<HTML>
<HEAD><TITLE> Notes on the Adaptive Mesh Refinement</TITLE></HEAD>
<BODY>
<h1>AMR</h1>
<p>
This document briefly describes the AMR-related features in MPI-AMRVAC. The different options can be set in the 
<A HREF="par.html#Amrlist">&amp;amrlist</A> of the <b>amrvac.par</b> 
file. For a more extensive description, you can read the article  
<em>`Parallel, grid-adaptive approaches for relativistic hydro and magnetohydrodynamics',
  R. Keppens, Z. Meliani, A.J. van Marle, P. Delmont, A. Vlasis, &amp B. van der Holst, 2011,
  JCP. </em><A HREF="http://dx.doi.org/10.1016/j.jcp.2011.01.020">doi:10.1016/j.jcp.2011.01.020</A>. 

<p>
MPI-AMRVAC uses a standard block-based, octree AMR scheme, where we have blocks of user-controlled dimension (set by the script <em>setamrvac -g=</em>) in a hierarchically nested manner. To simplify the parallelization, we gave up flexibility to allow different sized refinement ratios between grid levels, fixing it to 2. Also, we now use the same time step for all levels.
A generic skeleton code, generic enough to hold for any AMR code having similar restrictions, is shown below (in the subroutine terminology of MPI-AMRVAC it is shown <A HREF="amrvac_schematic.html">here</a>, where it corresponds with <em>timeintegration</em>).
<p>
<center><IMG SRC="figmovdir/skeleton.gif" HEIGHT=300></center>
<p>
Some more info follows on the different aspects involved.
<hr>
This page: 
	[<A HREF="#Parameters">Important (global) parameters</A>]
	[<A HREF="#Amrcrit">AMR criteria</A>]
	[<A HREF="#Data">Data structures</A>]
<hr>
<H2><A NAME="Parameters">Important (global) parameters</A></H2>

Some important global parameters are in the module <em>mod_indices.t</em>. In particular, note that the maximum number of grids per processor is the parameter <b>ngridshi</b>, and the maximum number of levels is the parameter <b>nlevelshi</b>. The latter is default set to 13. If your want to run with more levels, and/or allow for more grids per processor, you need to change their value and recompile. The number of levels set in the par/PROBLEM file as <em>mxnest</em> must always be smaller or equal to <b>nlevelshi</b>. This <b>nlevelshi</b> also returns in those parameters that are defined per level, such as <em>typefull1</em> which needs to be set for all (default) 13 levels.
<p>
<hr>
<H2><A NAME="Amrcrit">AMR criteria</A></H2>

<p>
This in essence describes the module <em>errest.t</em>, or at least its most essential aspets.
<p>
The block-tree nature implies that a decision for refining/coarsening is to be made on a block-by-block basis. This automated block-based regridding procedure involves 3 steps:
<pre>
(1) consider all blocks at level 1< l< <em>mxnest</em> , with <em>mxnest</em> the maximal grid level selected;
(2) quantify the local error E_\xx at each gridpoint in a certain grid block;
(3) if any point has this error exceeding a user-set tolerance <em>tol(l)</em>, refine this block (and ensure proper nesting);
(4) if all points have their error below a user-set fraction of the tolerance <em>tolratio(l)</em> used in the previous step, coarsen the block (for l>1). 
</pre>
The local error estimator can be one of three options, selected by <em>errorestimate</em>, each possibly augmented with user-defined criteria. Any of these estimators use a user-selected subset of the conserved or auxiliary variables (or even variables that are computed dynamically at the time of regridding), through the formula
<center><IMG SRC="figmovdir/error1.gif"></center>
The indices included are user-identified with the <em>flags</em> array, where <em>flags(nw+1)</em> says how many variables are included in the estimator, and then the first as many entries of <em>flags</em> identify the actual variable (our of the predefined order specific to the physics module selected). The corresponding weights are in <em>wflags</em>, and they must add up to one.
<p>
For <em>errorestimate=1</em>, a Richardson procedure computes 2 future solutions at t^(n+1), by using the t^(n-1) solution, coarsening it to a 2\Delta x_i grid, and then integrating this solution using 2\Delta t. This coarsened-integrated solution is denoted as w^{CI}. We also start with the t^n solution, integrate this with \Delta t, and subsequently coarsen it to 2\Delta x_i. This integrated-coarsened solution is referred to as w^{IC}. The local relative variable errors are then found from
<center><IMG SRC="figmovdir/error2.gif"></center>
The integrator used in this Richardson estimator is for our purposes a first order, dimensionally unsplit scheme, only incorporating unsplit source terms.
<p>
For <em>errorestimate=2</em>, 
a local comparison merely employs the availability of the t^{n-1} and t^n solution vectors. It estimates the local relative variable errors as
<center><IMG SRC="figmovdir/error3.gif"></center>
This is obviously computationally cheaper, but has the disadvantage that it in essence uses historical info, which may be insufficient for rapidly moving, strong shock cases. In our experience, both Richardson or local error estimators work satisfactorily on a variety of test problems, but both may need an added, user-set buffer zone around each grid point flagged for refinement in this manner. This zone sets the buffer width in numbers of grid cells (per dimension) <em>nbufferx1,...</em> about flagged grid cells.
<p>
For <em>errorestimate=3</em>, is the Lohner [R. Lohner,
   An adaptive finite element scheme for transient problems in CFD,
  Comp. Meth. App. Mech. Eng. 61, 323 (1987)]
prescription as adjusted in the PARAMESH library or the FLASH3 code. In our experience, it does not require any of the buffering just discussed, and is computationally efficient as it employs only instantaneous values from t^n.
It in essence discretizes a weighted second derivative in each grid point. In formulae,
<center><IMG SRC="figmovdir/error4.gif"></center>
where the operators mean a central difference and a sum, per dimension. The wave file parameter is set per level, and defaults as <em>amr_wavefilter(1:nlevelshi)=1.0d-2</em>.

<hr>
<H2><A NAME="Data">Data structures</A></H2>

<p>
The data structures are defined in <em>mod_physicaldata.t</em> and <em>mod_forest.t</em>, you can inspect them for learning more details.
<p>
We provide details on useful data structures. All of these are suited for any curvilinear coordinate system, and merely reflect the tree structure of the block-AMR. We implicitly assume a fixed refinement ratio of two. Schematic figures for a 2D Cartesian case generalize straightforwardly to higher or lower dimensionality.
<p>
The overall domain is considered `rectangular', i.e. bounded by coordinate pairs <em>xprobmin1,xprobmax1, ...</em> in each dimension. On the lowest grid level l=1, one controls the coarsest resolution as well as a suitable domain decomposition, by specifying both the total number of level 1 grid cells <em>nxlone1, ...</em> along with the individual block size per dimension (this is done by <em>setamrvac -g=</em>, which includes the ghost cells, a layer of width <em>dixB</em>). <b>The total cell number must be an integer multiple of the block size, so e.g. <em>nxlone1=20</em> and <em>dixB=2</em> for a 1D case requires <em>setamrvac -g=14</em>, which creates two grids of size 10 on level one each having a total of 4 ghost cells (2 on each side, hence 14).</b>
<p>
A hypothetical 2D domain is shown below, which corresponds to a domain where 4 by 3 blocks on level 1 are exploited in this domain decomposition, and where local refinement was activated in 4 out of these level l=1 blocks, here in the top right domain corner, as well as in one level l=2 grid. 
<center><IMG SRC="figmovdir/dataAA.gif"></center>
Global, integer grid indices are introduced per dimension, in a manner where knowledge of these grid indices, combined with AMR level knowledge, instantly allows one to localize the grid when needed. Following the figure, the grid on level l=2 indicated by global grid indices (5,3) is indeed the fifth grid block horizontally, and the third vertically, when the domain would be resolved fully with level l=2 blocks. The total amount of grid blocks per dimension, per level l, is stored in <em>ng^D(l)</em>, and the actual length of a grid block on level l, per dimension, is <em>dg^D(l)</em>.
<p>
The bottom figure reflects the tree representation of the same hypothetical grid hierarchy, where the presence of a grid leaf at a certain grid level is identified through a boolean variable. As indicated before, the total number of active grid leafs <em>nleafs</em> may change from timestep to timestep. This tree info is stored in the structure <em>tree_root(ig^D(l))</em>, which knows about the global grid index through <em>tree_root(ig^D(l))%node%ig^D</em>, the level <em>tree_root(ig^D(l))%node%level</em>, the processor on which it resides through the integer <em>tree_root(ig^D(l))%node%ipe</em>, and its presence or absence in the logical <em>tree_root(ig^D(l))%node%leaf</em>.
<center><IMG SRC="figmovdir/dataAB.gif"></center>
<p>
Various extra indices are helpful to traverse the tree structure. Local grid indices across AMR levels are schematically given below, which are used to identify the directional neighbours, as well as the children and parent blocks. These are used to realize and facilitate the possible interprocessor communication patterns, which are schematically shown at right.
<center><IMG SRC="figmovdir/dataA.gif" HEIGHT=200><IMG SRC="figmovdir/dataB.gif" HEIGHT=200><IMG SRC="figmovdir/dataD.gif" HEIGHT=200><IMG SRC="figmovdir/dataG.gif" HEIGHT=200></center>
<p>
The directional neighbours of a grid block are shown for a 1D, 2D and 3D case in the picture below.
<center><IMG SRC="figmovdir/dataC.gif"></center>
<p>
For parallelization, we adopted a fairly straightforward Z-order or Morton-order space filling curve. 
For the same hypothetical grid structure shown previously, the Morton space-filling curve is illustrated below, along with the resulting distribution of these 27 grid blocks on 4 CPUs. Load-balancing is done after every timestep, following the adaptive remeshing. When exploiting N_p CPUs, our strategy for load balancing merely ensures that each CPU has at least [nleafs/N_p]_int (denoting integer division) grid blocks, while the remainder increase this number by 1 for the first as many CPUs. In the example shown, this implies that the first 3 CPUs each contain 7 grid blocks, while the fourth has 6. 
The grid Morton numbers of all grids residing on processor <em>mype</em> lie between
<em>Morton_start(mype),Morton_stop(mype)</em>. The global grid index, once you know the grid Morton number <em>Morton_no</em> is found from <em>sfc_to_igrid(Morton_no)</em>, which gives the relation between the Space Filling Curve (sfc) and the global grid index <em>igrid</em>. The data for the conservative variables for grid <em>igrid</em> is then actually found from <em>pw(igrid)%w</em>.
<center><IMG SRC="figmovdir/dataF.gif"></center>
<p>
Some operations benefit from having a linear, linked list possibility to traverse the tree on a level by level basis. To that end, each grid also contains a pointer to the previous and next grid created in the same AMR level, taking all grids on all processors into account. This linked list is complemented with a globally known pointer to the first (head) and last (tail) grid on each level. For the hypothetical grid structure used above, this corresponding linked list representation is shown next.
<center><IMG SRC="figmovdir/dataE.gif"></center>
<p>
<hr>
</BODY>
</HTML>
