<HTML>
<HEAD>
<TITLE> MANUAL FOR DATA FILE CONVERSION </TITLE>
</HEAD>
<BODY>
<h1>DATA FILE CONVERSION</h1>

The standard <A HREF="fileformat.html">MPI-AMRVAC dataformat</A>, i.e. the <em>*.dat</em> files usable for restart, contain all the conservative variables in all gridblocks, and hence suffice for visualization, in principle. However, in many instances, one would like to use data formats that are directly readable by some of the more widespread visualization software packages. Therefore, we created the <em>convert.t</em> module, which ensures that this post-process data file conversion can be done with the same executable (but possibly even on a different platform). The many possibilities include conversion to <em>*.vtu</em> (VTK unformatted data format) directly readable by <A HREF="http://www.paraview.org/">Paraview</A> (or <A HREF="https://wci.llnl.gov/codes/visit/">ViSiT</A>), to <em>*.plt</em> format for the commercial package <A HREF="http://www.tecplot.com/">Tecplot</A>, or the <em>*.dx</em> format for the opensource package <A HREF="http://www.opendx.org/">openDX</A>. We also provide possibilities to convert to a format <em>*.out</em> suitable for Idl, for which some automated macro's can be made in analogy with those used for the <A HREF="http://grid.engin.umich.edu/~gtoth/VAC">Versatile Advection Code </A>, but be forewarned that 3D (and even 2D) AMR data for Idl will require you to program your own macro's.
Also, <b>this info will not explain you how to use the mentioned software for visualization, but just explain how to do the conversion.</b> Furthermore, this part of the code is subject to continuous change and improvement, and we welcome extra contributions.
<p>
We now give some brief info on how to use the same executable <em>amrvac</em> (which you already compiled and used to obtain output <em>*.dat</em> files with), to convert a single or all <em>*.dat</em> file(s) to one of these formats.
<hr>
This page:
 [<A HREF="#Converting">Converting on a single CPU</A>]
 [<A HREF="#PARConverting">Parallel conversion options</A>]
 [<A HREF="#Autoconvert">Autoconvert</A>]
 [<A HREF="#Notes">Some notes on extra conversion possibilities</A>]
 [<A HREF="#Endian">Endianness</A>]
<hr>

<H2><A NAME="Converting">Converting (on a single CPU)</A></H2>

<b> Note that all the steps below assume you're running on a single CPU. The same steps are to be taken for obtaining any of the other precoded data formats. One important warning is due: when you run a simulation for some reason twice, and you did not erase the previously created <em>*.dat</em> files, these files are overwritten (if the filenameout has not changed). Then, it may be that conversion fails, since the end of the file may contain some leftover data from the previous time, if the filelength has changed due to some other reason. The only remedy to this is that one should always remove old <em>*.dat</em> files, or never forget to change the name for the files accordingly, by setting <em>filenameout</em> in the <em>&filelist</em>.</b>

<p>
We will assume that you ran the standard 2D advection problem used for test purposes, i.e. that you did the following steps beforehand:
<pre>
cd $AMRVAC_DIR/tests/rho/vac
mkdir datamr
$AMRVAC_DIR/setup.pl -d=22 -phi=0 -z=0 -p=rho -g=16,16 -s
make
ln -s testrho_vac22 amrvac.par
mpirun -np 1 amrvac
</pre>
We also assume that in the parameter file mentioned above, the namelist <em>&filelist</em> was stating (note that the end of the namelist is indicated as usual by a backslash)
<pre>
 &filelist
        filenamelog='datamr/vaclogo'
        filenameout='datamr/vaclogo'
        primnames='rho'
 /
</pre>
If all went well, you then have created as many <em>*.dat</em> files as requested through the settings you provided in the combined <em>&savelist</em> and <em>&stoplist</em> namelists from the <A HREF="par.html">par-file</A>. For the example, they normally default to asking a full data dump at time zero, as well as every time the time has increased by 0.05, and this till <em>tmax=1.0d0</em>, such that we actually have 21 snapshots in total. You should thus have files like <em>datamr/vaclogo0000.dat</em> up to  <em>datamr/vaclogo0020.dat</em>. You can now individually convert such <em>*.dat</em> file to a <em>*.vtu</em> file by doing the following. Edit the par-file, to modify the <em>&filelist</em> to something like
<pre>
 &filelist
        filenamelog='datamr/vaclogo'
        filenameout='datamr/vaclogo'
        primnames='rho'
        filenameini='datamr/vaclogo'
        convert=.true.
        convert_type='vtuCC'
        saveprim=.false.
        snapshotini=0
 /
</pre>
Assuming that this par-file is still known through the symbolic link <em>amrvac.par</em> as above, you can then convert a single <em>*.dat</em> file (here the <em>datamr/testrho/vaclogo0000.dat</em> file, as we select <em>snapshotini=0</em>) simply running again
<pre>
mpirun -np 1 amrvac
</pre>
or, which is actually equivalent (single CPU)
<pre>
amrvac
</pre>
Note that this will create a new file, namely <em>datamr/vaclogo0000.vtu</em>, which can be directly imported in Paraview. It will, under the settings above, just contain the density on the grid hierarchy at time zero. The <em>convert_type='vtuCC'</em> indicates that the data is exactly as the code interprets and updates the values, namely as cell-centered quantities. The <em>saveprim=.false.</em> has for the example here no real meaning, as for advection conservative and primitive variables coincide (just density <em>rho</em> exists).
<p>
Realizing that you typically want to convert multiple data files, you can do this by repeating the above as many times as here are <em>*.dat</em> files, by raising/changing the <em>snapshotini</em> identifier. Since you typicallly want to convert all data files between a minimum and maximum number of similarly named files, the script <b>doconvert</b> is added. If you have a line <code>PATH="$AMRVAC_DIR:$AMRVAC_DIR/tools:./:$PATH"</code> in <code>~/.bash_profile</code> (or <code>~/.bashrc</code>), typing <em>doconvert</em> will tell you its intended usage, namely
<pre>
doconvert testrho_vac22 0 20
</pre>
in the example case at hand, where we created 21 data files from running the advection problem. <b>This <em>doconvert</em> script does assume that you actually edited the par-file manually once as above (such that the needed lines for conversion are in the <em>&filelist</em> namelist), and that the executable <em>amrvac</em> exists in the same directory.</b>
It will complain when the parfile does not exist, and obviously requires the existence of all files between the start and stopindex (0 and 20 here). With paraview, you will then be able to immediately import all 21 <em>*.vtu</em> files with the same base filename, and directly make movies or still images from them. Furthermore, there is a more convenient script <b>aiconvert</b>, which will automatically modify the par-file, do converting data, and resume the par-file, as long as the amrvac.par has been linked and it has the parameters for converting like
<pre>
 &filelist
        filenamelog='datamr/vaclogo'
        filenameout='datamr/vaclogo'
        primnames='rho'
 /
        filenameini='datamr/vaclogo'
        convert=.true.
        convert_type='vtuCC'
        saveprim=.false.
        snapshotini=0

 &savelist
</pre>
For example, to convert snapshots from number 10 to number 20:
<pre>
aiconvert 10 20
</pre>
or to convert the snapshot number 12
<pre>
aiconvert 12
</pre>
or just type
<pre>
aiconvert
</pre>
to convert all the snapshots! Besides, if amrvac.par is not linked just do similarly as doconcert:
<pre>
aiconvert testrho_vac22 0 20
</pre>
For details of aiconvert, please read the header of the $AMRVAC_DIR/tools/aiconvert.

<hr>
<H2><A NAME="PARConverting">Parallel conversion options</A></H2>

For very large simulations (typically 3D, and/or runs achieving high effective resolutions), even the data conversion may need to be done in parallel (and ultimately, the visualization itself too). The <em>convert.t</em> allows to perform some of the <em>*.dat</em> conversions in parallel, in particular, this is true for the <em>*.vtu</em> format, and for the <em>*.plt</em> format. You should then select one of
<pre>
convert_type='vtumpi'
convert_type='vtuCCmpi'
convert_type='vtuBmpi'
convert_type='vtuBCCmpi'
convert_type='vtimpi'
convert_type='vtiCCmpi'
convert_type='pvtumpi'
convert_type='pvtuCCmpi'
convert_type='pvtuBmpi'
convert_type='pvtuBCCmpi'
convert_type='tecplotmpi'
convert_type='tecplotCCmpi'
</pre>
Here, the prefix <em>p</em> stands for the parallel file format, where each process is allowed to dump its data into a single (e.g. <em>*.vtu</em>) file and a master file (e.g. <em>*.pvtu</em>) is stored by rank zero. This has the advantage that the write operation on suitable file systems is sped up significantly.  In a visualization software, only the <em>*.pvtu</em> files need to be imported and also the reading process is sped up in case of  parallel visualization.  
<p>
Also, you can then use the same strategy as explained above for converting on a single CPU: you will always need to edit the <A HREF="par.html#Filelist">par-file</a> once to specify how to do the conversion, and then you may run interactively on e.g. 4 CPU like
<pre>
mpirun -np 4 amrvac
</pre>
or do this in batch (use a batch job script for that), to do multiple data file conversions. We also provide a small script, called <b>doconvertpar</b>, which works similar to the <b>doconvert</b> explained above, but takes one extra parameter: the number of CPUs. Its usage is described by
<pre>
doconvertpar parfilename startindex stopindex nprocessor
</pre>
Besides, you can again use aiconvert as expained above, and type in the number of processors to use by answering a popup question:
<pre>
How many processors do you want to use? (default=1) 4
</pre>
<hr>
<H2><A NAME="Autoconvert">Autoconvert</A></H2>

In addition to the conversion after the run, AMRVAC now offers also to directly output files ready to use for visualization along with the simulation.  A parallel run will however only be capable to provide the file-types ready for parallel conversion (see <A HREF="#PARConverting">parallel conversion</A>).  To enable this capability, simply set the switch <em>autoconvert=.true.</em>.
The example above would then read
<pre>
&filelist
        filenamelog='datamr/testrho/vaclogo'
        filenameout='datamr/testrho/vaclogo'
        primnames='rho'
        saveprim=.false.
        autoconvert=.true.
        convert_type='pvtuCCmpi'
 /
</pre>
and when the code is run via
<pre>
mpirun -np 2 amrvac
</pre>
three new output files (<em>vaclogoXXXX.pvtu, vaclogoXXXXp0000.vtu, vaclogoXXXXp0001.vtu</em>) will appear simultaneous to the <em>vaclogoXXXX.dat</em> files, stored at given intervals.  All functionality of the usual convert is conserved, e.g. derived quantities and primitive variables (using the <em>saveprim=.true.</em> option) can be stored in the output files.  


<hr>
<H2><A NAME="Notes">Notes on conversion possibilities</A></H2>
<h3>Cell center versus cell corner values</h3>
In all cases mentioned below, the difference between convert-types with or without <em>CC</em> relates to the difference between cell center (`CC') versus cell corner values. For the cell center case, no interpolation of the computed data is needed, since the (conservative) variables actually represent volume averages stored at cell centers. For the other cases (without 'CC'), the <em>convert.t</em> tries to interpolate from cell center to the cell corners, and then the data will be known at the grid nodes. This will be noticable on reading in the data in <em>paraview</em>, which reports whether data is cell data (cell centered) or point data (nodes or corners). In principle, the conversion from cell centered (or cell data) to cell corner (or point data) types can also be achieved in paraview itself, with the filter <em>Cell data to Point data</em>. There may be subtle differences between the way MPI-AMRVAC does this nterpolation, and the way it happens internally to paraview, so we provide both options as output <em>*.vtu</em> files. Similar observations hold for the Tecplot format.
<h3>Conservative/primitive storage and adding derived quantities</h3>
The <b>saveprim</b> logical allows you to select whether the conservative or primitive variables need to be stored in the resulting output file. The names for the conservative variables are taken from the <em>wnames</em> string, and those for the primitive need to be set in <em>primnames</em>. 
<p>
Another very useful option is to specify which variables actually need to be converted: by default all conservative variables available in the <em>*.dat</em> file will be included, but then again filesizes may become restrictive. For that purpose, the logical array <em>writew</em> allows to select which variable(s) to store (and this in combination with saveprim, possibly). You can then create different files for selected variables, knowing that the output filename will start with <em>filenameout</em>, while the actual data file converted is known from the combination <em>filenameini</em> and <em>snapshotini</em>.
<p>
We allow the possibility to compute derived variables from the <em>*.dat</em> file in the userfile, by setting how many you add beyond the <em>nw</em> variables typcial for the physics module at hand, in the integer <em>nwauxio</em>. Correspondingly that many variables, you should then compute and store in the <em>w(*,nw+1)</em> ...  <em>w(*,nw+nwauxio)</em>  entries, in the user-written subroutine <em> specialvar_output</em> (as defined in <em>amrvacnul.speciallog.t</em>). The names for these variables then need to be provided in the corresponding <em>specialvarnames_output</em> subroutine, which simply then extends the strings <em>wnames</em> and <em>primnames</em>. This feature is very useful, for the same reason as above: you can let the code compute gradients of scalar fields, divergence of vector quantities, curls of vectors, etc, using the precoded subroutines for that purpose found in <em>geometry.t</em>. You then do not rely on visualization software to do interpolations or discretizations, which may not reflect those actually taken in MPI-AMRVAC.
<p>
Another useful feature is the possibility to select the output AMR level. You can let the code compute from the <em>*.dat</em> file an output residing on a specified level <em>level_io</em>. This then uses the MPI-AMRVAC internal means to perform restriction and prolongations, and you can then make sure to have a single uniform grid output too.
<h3><em>convert_type='vtu'</em> or <em>convert_type='vtuCC'</em></h3>
Does the conversion to <em>*.vtu</em> data files. This option works on 1 CPU. The resulting <em>*.vtu</em> files contain data in ASCII format. 
<h3><em>convert_type='vtuB'</em> or <em>convert_type='vtuBCC'</em></h3>
Does the conversion to <em>*.vtu</em> data files. This option works on 1 CPU. The resulting <em>*.vtu</em> files are in binary format.
<h3><em>convert_type='vtumpi'</em> or <em>convert_type='vtuCCmpi'</em></h3>
Does the conversion to <em>*.vtu</em> data files. This option works on multiple CPUs. The resulting <em>*.vtu</em> files contain data in ASCII format.
<h3><em>convert_type='vtuBmpi'</em> or <em>convert_type='vtuBCCmpi'</em></h3>
Does the conversion to <em>*.vtu</em> data files. This option works on multiple CPUs. The resulting <em>*.vtu</em> files contain data in binary format. (recommended)
<h3><em>convert_type='tecplot'</em> or <em>convert_type='tecplotCC'</em></h3>
This realizes conversion to <em>*.plt</em> files, which can be read in directly by Tecplot. Note that the output is in ASCII format, which may mean huge data sets, but Tecplot has its own <b>preplot</b> command that will allow you to operate on such a file, and thereby make a binary version for it. The above is for single CPU execution, allows to add user-defined variables with <em>nwauxio</em>, and renormalization using the <em>normt</em> and <em>normvar</em> array.

<h3><em>convert_type='tecplotmpi'</em> or <em>convert_type='tecplotCCmpi'</em></h3>
Same as above, but allows to perform the conversion in parallel. One can add user
-defined variables with <em>nwauxio</em>, and renormalize using the <em>normt</em> and <em>normvar</em> array. The current implementation is such that tecplotmpi and tecplotCCmpi will create different length output ASCII files when used on 1 versus multiple CPUs.
In fact, on 1 CPU, there will be as many (tecplot) zones as there are levels, while on
on multiple CPUs, there will be a number of zones up to the number of
levels times the number of CPUs (or less, when some level does not exist on a certain CPU).

<h3><em>convert_type='idl'</em> or <em>convert_type='idlCC'</em></h3>
This will do the conversion to <em>*.out</em> files, which are a generalization of the 
<A HREF="http://grid.engin.umich.edu/~gtoth/VAC">Versatile Advection Code </A> output files. For those VAC-style files, extensive macro's are provided with the VAC code itself, allowing for fairly interactive visualization of quantities, or computation of derived quantities etc. The Idl macro's that allow to read <em>*.out</em> files converted with MPI-AMRVAC, and that use similar commands as for VAC files, are downloadable <A HREF="allidlmacros.tar.gz">here as a single gzipped tar file</A>. It contains the hidden, to be adjusted file <em>.idlrc</em>, and the directories <em>Idl</em> and <em>Idl_amr</em>, with macro's inside. However, they only allow a very limited visualization, with some (possibly incomplete and inaccurate) <A HREF="idl.html">description here</a>, fine for 1D and little for 2D runs, but with no support for 3D data analysis. This would mean you need to write your own Idl macro's, after you fully understand what dataformat is actually stored in a <em>*.out</em> file. For that, just study the source code in <em>convert.t</em>. 
<p>
The Idl conversion does not work in parallel, it can handle the addition of extra IO variables (<em>nwauxio</em>), and allows to renormalize the data using the <em>normt</em> and <em>normvar</em> array, in case you directly want to have dimensional quantities available. An additional script provided is the <b>doidlcat</b> script, which basically concatenates all requested <em>*.out</em> files in a single file, that can be used with the (few) Idl macro's above. Its intended use for the 2D advection example would be
<pre>
doconvert par/testrho/testrho_vac22 0 20
doidlcat datamr/testrho/vaclogo 0 20 1
</pre>
The first line creates the 21 files <em>datamr/testrho/vaclogo0000.out</em> till
<em>datamr/testrho/vaclogo0020.out</em> (assuming you edited the par-file and indicated the proper <em>convert_type</em> for idl), while the next line then gathers them all in a single
<em>datamr/testrho/vaclogoall.out</em> file, ready for Idl visualization with VAC-like macro's, like <em>.r getpict</em>, <em>.r plotfunc</em> or <em>.r animate</em>. The 3 integer parameters to <b>doidlcat</b> indicate the first and last snapshot number, and a skiprate. If the latter is different from 1, you include every so many files in the concatenation.
<h3><em>convert_type='dx'</em></em></h3>
This (heritage) output format is suited for openDX conversion.
<h3>onegrid(mpi), oneblock(B), ...</h3>
Extra possibilities to allow creation of a single uniform grid level output. Please inspect the workings in <em>convert.t</em>. 
<hr>
<H2><A NAME="Endian">Endianness</A></H2>

Our <em>*.dat</em> files contain the data in binary format. This is in principle exchangeable between different platforms, with the possible exception for big endian or little
endian type machines (these differ in the ordering of bytes for integers and reals). E.g. IBM SP platforms have different endianness than the VIC3 cluster at K.U.Leuven. In that sense, you may need to do conversion on the same (type) platform than you used for running the code.
<p>
For the binary <em>.vtu</em> files, the endianness can be provided in the xml header and vtu readers like paraview will then interprete the data correctly.  The default endianness of <em>.vtu</em> files is little endian.  To switch to big endian, simply add the line
<pre>#define BIGENDIAN</pre>
to the <em>definitions.h</em> file.  
<p>
There can be solutions on the machine at hand, using the <em>assign</em> command (whose syntax you will need to get info on). We would also like to hear if anyone knows about a way to specify the endianness of the output in MPI/Fortran itself, independent of the platform.

Using Fortran compilers like gfortran and intel fortran, it is now possible to output .dat data files in the other endian, e. g. big endian, with a parameter
 endian_swap=.true. in the filelist section of the parameter file.


<hr>
</BODY>
</HTML>

